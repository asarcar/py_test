{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12.head\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow log verbosity: DEBUG, INFO, WARN, ERROR, and FATAL\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "# Tensorflow version\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Create 100 phony x, y data points in NumPy, y = x * 0.1 + 0.3\n",
    "x_data = np.random.rand(100).astype(np.float32)\n",
    "y_data = x_data * 0.1 + 0.3\n",
    "# Try to find values for W and b that compute y_data = W * x_data + b\n",
    "# (We know that W should be 0.1 and b 0.3, but TensorFlow will\n",
    "# figure that out for us.)\n",
    "W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))\n",
    "b = tf.Variable(tf.zeros([1]))\n",
    "y = W * x_data + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Minimize the mean squared errors.\n",
    "loss = tf.reduce_mean(tf.square(y - y_data))\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5)\n",
    "train_op = optimizer.minimize(loss)\n",
    "# Before starting, initialize the variables.  We will 'run' this first.\n",
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Launch the graph.\n",
    "sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n",
    "                                        log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "sess.run(init_op)\n",
    "# Fit the line.\n",
    "for step in range(201):\n",
    "    sess.run(train_op)\n",
    "    if step % 20 == 0:\n",
    "      print(\"#{0}:  W={1}: b={2}\".format(step, sess.run(W), sess.run(b)))\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 14.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "input1 = tf.placeholder(tf.float32)\n",
    "input2 = tf.placeholder(tf.float32)\n",
    "output = tf.mul(input1, input2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  print(sess.run([output], feed_dict={input1:[7.], input2:[2.]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# WARNING:  Disable GPU version 'sudo prime-select intel'  and restart kernel.\n",
    "# Otherwise: Error Message 'InvalidArgumentError (see above for traceback):\n",
    "# WhereOp: Race condition between counting ...'\n",
    "#\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.contrib.learn.python.learn.metric_spec import MetricSpec\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Tensorflow log verbosity: DEBUG, INFO, WARN, ERROR, and FATAL\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "gIrisDir = \"/home/data/asarcar-data/tmp/IRIS\"\n",
    "gIrisModelDir = gIrisDir + \"/model\"\n",
    "gIrisDataDir = gIrisDir + \"/data\"\n",
    "# Data sets\n",
    "IRIS_TRAINING = gIrisDataDir + \"/iris_training.csv\"\n",
    "IRIS_TEST = gIrisDataDir + \"/iris_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Load datasets.\n",
    "training_set = tf.contrib.learn.datasets.base.load_csv_with_header(\n",
    "    filename=IRIS_TRAINING,\n",
    "    target_dtype=np.int,\n",
    "  features_dtype=np.float32)\n",
    "test_set = tf.contrib.learn.datasets.base.load_csv_with_header(\n",
    "    filename=IRIS_TEST,\n",
    "    target_dtype=np.int,\n",
    "    features_dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print(type(test_set))\n",
    "print(test_set.data)\n",
    "print(type(test_set.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# validation_metrics = {\"accuracy\": tf.contrib.metrics.streaming_accuracy,\n",
    "#                      \"precision\": tf.contrib.metrics.streaming_precision,\n",
    "#                       \"recall\": tf.contrib.metrics.streaming_recall}\n",
    "\n",
    "# Evaluate every 50 steps against test data to track\n",
    "# how well the model is generalizing\n",
    "validation_metrics = {\n",
    "  \"accuracy\" : MetricSpec(\n",
    "    metric_fn=tf.contrib.metrics.streaming_accuracy,\n",
    "    prediction_key=\"classes\"),\n",
    "  \"precision\" : MetricSpec(\n",
    "    metric_fn=tf.contrib.metrics.streaming_precision,\n",
    "    prediction_key=\"classes\"),\n",
    "  \"recall\" : MetricSpec(\n",
    "    metric_fn=tf.contrib.metrics.streaming_recall,\n",
    "    prediction_key=\"classes\")\n",
    "}\n",
    "\n",
    "validation_monitor = tf.contrib.learn.monitors.ValidationMonitor(\n",
    "  test_set.data,\n",
    "  test_set.target,\n",
    "  every_n_steps=50,\n",
    "  metrics=validation_metrics,\n",
    "  early_stopping_metric=\"loss\",\n",
    "  early_stopping_metric_minimize=True,\n",
    "  early_stopping_rounds=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Specify that all features and data type ( real-value) data\n",
    "# Features:\n",
    "#   Sepal Length, Sepal Width, Petal Length, and Petal Width\n",
    "# Target:\n",
    "#   Species of the IRIS: 0 - Iris Setosa, 1 Iris Versicolor, or 2 - Iris Virginica\n",
    "feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=4)]\n",
    "# ValidationMonitors rely on saved checkpoints to perform evaluation.\n",
    "# Checkpoint every 1 sec\n",
    "# Build 3 layer DNN with 10, 20, 10 units respectively.\n",
    "classifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,\n",
    "                                            hidden_units=[10, 20, 10],\n",
    "                                            n_classes=3,\n",
    "                                            model_dir=gIrisModelDir,\n",
    "                                            config=tf.contrib.learn.RunConfig(save_checkpoints_secs=1))\n",
    "# Fit model: attaching monitors\n",
    "classifier.fit(x=training_set.data,\n",
    "               y=training_set.target,\n",
    "               steps=10000,\n",
    "               monitors=[validation_monitor])\n",
    "\n",
    "# Evaluate accuracy.\n",
    "accuracy_score = classifier.evaluate(x=test_set.data,\n",
    "\t\t\t\t     y=test_set.target)[\"accuracy\"]\n",
    "print('Accuracy: {0:f}'.format(accuracy_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Classify two new flower samples.\n",
    "new_samples = np.array(\n",
    "    [[6.4, 3.2, 4.5, 1.5], [5.8, 3.1, 5.0, 1.7]], dtype=float)\n",
    "y = list(classifier.predict(new_samples, as_iterable=True))\n",
    "print('Predictions: {}'.format(str(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# WARNING:  Disable GPU version 'sudo prime-select intel'  and restart kernel.\n",
    "# Otherwise: Error Message 'InvalidArgumentError (see above for traceback):\n",
    "# WhereOp: Race condit\n",
    "#\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from tensorflow.contrib.learn.python.learn.metric_spec import MetricSpec\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Tensorflow log verbosity: DEBUG, INFO, WARN, ERROR, and FATAL\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "gIrisDir = \"/home/data/asarcar-data/tmp/IRIS\"\n",
    "gIrisModelDir = gIrisDir + \"/model\"\n",
    "gIrisDataDir = gIrisDir + \"/data\"\n",
    "gTrainSteps = 2000\n",
    "# Data sets\n",
    "IRIS_TRAINING = gIrisDataDir + \"/iris_training.csv\"\n",
    "IRIS_TEST = gIrisDataDir + \"/iris_test.csv\"\n",
    "IRIS_PREDICTION = gIrisDataDir + \"/iris_prediction.csv\"\n",
    "#\n",
    "# HEADER FORMAT\n",
    "#   num_rows, num_features, target_classes\n",
    "#   120,4,setosa,versicolor,virginica\n",
    "# DATA FORMAT\n",
    "#   Sepal Length (1), Sepal Width(2), Petal Length(3), Petal Width(4), Species(5)\n",
    "#     Species: 0 - Iris Setosa, 1 Iris Versicolor, or 2 - Iris Virginica\n",
    "#   6.4,2.8,5.6,2.2,2\n",
    "#\n",
    "COLUMNS = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"species\"]\n",
    "FEATURES = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n",
    "LABEL = \"species\"\n",
    "gNumClasses = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Load datasets.\n",
    "training_set = pd.read_csv(IRIS_TRAINING, skipinitialspace=True,\n",
    "\t\t\t   skiprows=1,names=COLUMNS)\n",
    "test_set = pd.read_csv(IRIS_TEST, skipinitialspace=True,\n",
    "\t\t       skiprows=1,names=COLUMNS)\n",
    "prediction_set = pd.read_csv(IRIS_PREDICTION, skipinitialspace=True,\n",
    "\t\t\t     skiprows=1,names=COLUMNS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def input_fn(df):\n",
    "  # Creates a dictionary mapping from each continuous feature column name (k) to\n",
    "  # the values of that column stored in a constant Tensor.\n",
    "  feature_cols = {k: tf.constant(df[k].values) for k in COLUMNS}\n",
    "  # Converts the label column into a constant Tensor.\n",
    "  label = tf.constant(df[LABEL].values)\n",
    "  # Returns the feature columns and the label.\n",
    "  return feature_cols, label  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Specify that all features and data type ( real-value) data\n",
    "feature_cols = [tf.contrib.layers.real_valued_column(k) for k in FEATURES]\n",
    "# Build 3 layer DNN with 10, 20, 10 units respectively.\n",
    "classifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_cols,\n",
    "                                            hidden_units=[10, 20, 10],\n",
    "                                            n_classes=gNumClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Fit model: attaching monitors\n",
    "classifier.fit(input_fn = lambda: input_fn(training_set), steps=gTrainSteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate accuracy.\n",
    "eval_result = classifier.evaluate(input_fn = lambda: input_fn(test_set), steps=1)\n",
    "print('Accuracy: {0:f}'.format(eval_result[\"accuracy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def predict_input_fn():\n",
    "  # Bug in Predict Code when passing function as input_fn\n",
    "  new_samples = {\n",
    "    \"sepal_length\": tf.constant(np.array([6.4,5.8], dtype=np.float64)),\n",
    "    \"sepal_width\": tf.constant(np.array([3.2,3.1], dtype=np.float64)),\n",
    "    \"petal_length\": tf.constant(np.array([4.5,5.0], dtype=np.float64)),\n",
    "    \"petal_width\": tf.constant(np.array([1.5,1.7], dtype=np.float64))\n",
    "  }\n",
    "  # FEATURES = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n",
    "  new_result = tf.constant(np.array([1,2], dtype=np.int64))\n",
    "  return new_samples, new_result\n",
    "es = classifier.predict(input_fn = predict_input_fn)\n",
    "predictions = list(itertools.islice(res, 2))\n",
    "print (\"Predictions: {}\".format(str(predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Data Format\n",
    "# FEATURES:\n",
    "# ----------------\n",
    "# CRIM\tCrime rate per capita\n",
    "# ZN\tFraction of residential land zoned to permit 25,000+ sq ft lots\n",
    "# INDUS\tFraction of land that is non-retail business\n",
    "# NOX\tConcentration of nitric oxides in parts per 10 million\n",
    "# RM\tAverage Rooms per dwelling\n",
    "# AGE\tFraction of owner-occupied residences built before 1940\n",
    "# DIS\tDistance to Boston-area employment centers\n",
    "# TAX\tProperty tax rate per $10,000\n",
    "# PTRATIO\tStudent-teacher ratio\n",
    "#\n",
    "# LABEL:\n",
    "# ----------\n",
    "# MEDV Median value of owner-occupied residences in $1,000s\n",
    "COLUMNS = [\"crim\", \"zn\", \"indus\", \"nox\", \"rm\", \"age\",\n",
    "           \"dis\", \"tax\", \"ptratio\", \"medv\"]\n",
    "FEATURES = [\"crim\", \"zn\", \"indus\", \"nox\", \"rm\",\n",
    "            \"age\", \"dis\", \"tax\", \"ptratio\"]\n",
    "LABEL = \"medv\"\n",
    "gUCIdir = \"/home/data/asarcar-data/tmp/UCI/\"\n",
    "gUCIdata = gUCIdir + \"data/\"\n",
    "gUCImodel = gUCIdir + \"model/\"\n",
    "gTrainFileName = gUCIdata + \"boston_train.csv\"\n",
    "gTestFileName = gUCIdata + \"boston_test.csv\"\n",
    "gTrainSteps = 5000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "training_set = pd.read_csv(gTrainFileName, skipinitialspace=True,skiprows=1, names=COLUMNS)\n",
    "test_set = pd.read_csv(gTestFileName, skipinitialspace=True,skiprows=1, names=COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "feature_cols = [tf.contrib.layers.real_valued_column(k) for k in FEATURES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "regressor = tf.contrib.learn.DNNRegressor(\n",
    "  feature_columns = feature_cols, hidden_units=[10,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Build Input Function for Transforming Raw Data and Feeding to DNN\n",
    "def input_fn(data_set):\n",
    "  feature_cols = {k: tf.constant(data_set[k].values) for k in FEATURES}\n",
    "  labels = tf.constant(data_set[LABEL].values)\n",
    "  return feature_cols, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "regressor.fit(input_fn=lambda:input_fn(training_set),steps=gTrainSteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "ev = regressor.evaluate(input_fn=lambda:input_fn(test_set),steps=1)\n",
    "loss_score = ev[\"loss\"]\n",
    "print(\"Loss: {0:f}\".format(loss_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "y = regressor.predict(input_fn=lambda: input_fn(predict_set))\n",
    "# .predict returns an iterator => convert to list\n",
    "predictions = list(itertools.islice(y, 6))\n",
    "print (\"Predictions: {}\".format(str(predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# WARNING: Disable GPU version 'sudo prime-select intel'  and restart kernel.\n",
    "# Otherwise: Error Message InvalidArgumentError: WhereOp: Race condition\n",
    "# between counting the number of true elements and writing them.\n",
    "# When counting, saw xxxx elements; but when writing their indices,\n",
    "# saw xx elements.\n",
    "#\n",
    "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"Example code for TensorFlow Wide & Deep Tutorial using TF.Learn API.\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from six.moves import urllib\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Tensorflow log verbosity: DEBUG, INFO, WARN, ERROR, and FATAL\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Parameter Globals\n",
    "gCensusDir = \"/home/data/asarcar-data/tmp/CENSUS/\"\n",
    "# 'Initial learning rate.'\n",
    "gInputDataDir           =  gCensusDir + \"data/\"\n",
    "# \"Base directory for output models.\"\n",
    "gModelDir = gCensusDir + \"model/\"\n",
    "# Model Types: \"wide\",  \"deep\",  \"wide_n_deep\"\n",
    "gModelType = \"wide_n_deep\"\n",
    "# \"Number of training steps.\"\n",
    "gTrainSteps = 200\n",
    "# URL Base where data directory\n",
    "gURLBase = \"http://mlr.cs.umass.edu/ml/machine-learning-databases/adult/\"\n",
    "# TrainFile\n",
    "gTrainFile = \"adult.data\"\n",
    "# TestFile\n",
    "gTestFile = \"adult.test\"\n",
    "# Train File URL\n",
    "gTrainFileURL = gURLBase + gTrainFile\n",
    "# Test File URL\n",
    "gTestFileURL = gURLBase + gTestFile\n",
    "# Train FileName\n",
    "gTrainFileName = gInputDataDir + gTrainFile\n",
    "# Test FileName\n",
    "gTestFileName = gInputDataDir + gTestFile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# CENSUS DATA FORMAT\n",
    "COLUMNS = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\",\n",
    "           \"marital_status\", \"occupation\", \"relationship\", \"race\", \"gender\",\n",
    "           \"capital_gain\", \"capital_loss\", \"hours_per_week\", \"native_country\",\n",
    "           \"income_bracket\"]\n",
    "LABEL_COLUMN = \"label\"\n",
    "CATEGORICAL_COLUMNS = [\"workclass\", \"education\", \"marital_status\",\n",
    "\t\t       \"occupation\",\"relationship\", \"race\", \"gender\",\n",
    "\t\t       \"native_country\"]\n",
    "CONTINUOUS_COLUMNS = [\"age\", \"education_num\", \"capital_gain\",\n",
    "\t\t      \"capital_loss\",\"hours_per_week\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def maybe_download():\n",
    "  \"\"\"Maybe downloads training data and returns train and test file names.\"\"\"\n",
    "  # pylint: disable=line-too-long\n",
    "  urllib.request.urlretrieve(gTrainFileURL, gTrainFileName)  \n",
    "  print(\"Training data is downloaded to %s\" % gTrainFileName)\n",
    "  urllib.request.urlretrieve(gTestFileURL, gTestFileName)  \n",
    "  print(\"Test data is downloaded to %s\" % gTestFileName)\n",
    "  return gTrainFileName, gTestFileName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def build_estimator(model_dir):\n",
    "  \"\"\"Build an estimator.\"\"\"\n",
    "  # Sparse base columns.\n",
    "  gender = tf.contrib.layers.sparse_column_with_keys(column_name=\"gender\",\n",
    "                                                     keys=[\"female\", \"male\"])\n",
    "  education = tf.contrib.layers.sparse_column_with_hash_bucket(\n",
    "      \"education\", hash_bucket_size=1000)\n",
    "  relationship = tf.contrib.layers.sparse_column_with_hash_bucket(\n",
    "      \"relationship\", hash_bucket_size=100)\n",
    "  workclass = tf.contrib.layers.sparse_column_with_hash_bucket(\n",
    "      \"workclass\", hash_bucket_size=100)\n",
    "  occupation = tf.contrib.layers.sparse_column_with_hash_bucket(\n",
    "      \"occupation\", hash_bucket_size=1000)\n",
    "  native_country = tf.contrib.layers.sparse_column_with_hash_bucket(\n",
    "      \"native_country\", hash_bucket_size=1000)\n",
    "  # Continuous base columns.\n",
    "  age = tf.contrib.layers.real_valued_column(\"age\")\n",
    "  education_num = tf.contrib.layers.real_valued_column(\"education_num\")\n",
    "  capital_gain = tf.contrib.layers.real_valued_column(\"capital_gain\")\n",
    "  capital_loss = tf.contrib.layers.real_valued_column(\"capital_loss\")\n",
    "  hours_per_week = tf.contrib.layers.real_valued_column(\"hours_per_week\")\n",
    "\n",
    "  # Transformations.\n",
    "  age_buckets = tf.contrib.layers.bucketized_column(age,\n",
    "                                                    boundaries=[\n",
    "                                                        18, 25, 30, 35, 40, 45,\n",
    "                                                        50, 55, 60, 65\n",
    "                                                    ])\n",
    "  # L1 regularization tends to make model weights stay at zero,\n",
    "  # creating sparser models, whereas L2 regularization also tries\n",
    "  # to make the model weights closer to zero but not necessarily zero.\n",
    "  # Strong L1 regularization => smaller model size:\n",
    "  # Desirable when feature space is large & sparse or resource constraints\n",
    "  opt = tf.train.FtrlOptimizer(\n",
    "    learning_rate=0.1,\n",
    "    l1_regularization_strength=1.0,\n",
    "    l2_regularization_strength=1.0)\n",
    "  # Wide columns and deep columns.\n",
    "  wide_columns = [gender, native_country, education, occupation, workclass,\n",
    "                  relationship, age_buckets,\n",
    "                  tf.contrib.layers.crossed_column([education, occupation],\n",
    "                                                   hash_bucket_size=int(1e4)),\n",
    "                  tf.contrib.layers.crossed_column(\n",
    "                      [age_buckets, education, occupation],\n",
    "                      hash_bucket_size=int(1e6)),\n",
    "                  tf.contrib.layers.crossed_column([native_country, occupation],\n",
    "                                                   hash_bucket_size=int(1e4))]\n",
    "  deep_columns = [\n",
    "      tf.contrib.layers.embedding_column(workclass, dimension=8),\n",
    "      tf.contrib.layers.embedding_column(education, dimension=8),\n",
    "      tf.contrib.layers.embedding_column(gender, dimension=8),\n",
    "      tf.contrib.layers.embedding_column(relationship, dimension=8),\n",
    "      tf.contrib.layers.embedding_column(native_country,\n",
    "                                         dimension=8),\n",
    "      tf.contrib.layers.embedding_column(occupation, dimension=8),\n",
    "      age,\n",
    "      education_num,\n",
    "      capital_gain,\n",
    "      capital_loss,\n",
    "      hours_per_week,\n",
    "  ]\n",
    "\n",
    "  if gModelType == \"wide\":\n",
    "    m = tf.contrib.learn.LinearClassifier(model_dir=model_dir,\n",
    "\t\t\t\t\t  feature_columns=wide_columns,\n",
    "\t\t\t\t\t  optimizer = opt)\n",
    "  elif gModelType == \"deep\":\n",
    "    m = tf.contrib.learn.DNNClassifier(model_dir=model_dir,\n",
    "                                       feature_columns=deep_columns,\n",
    "                                       hidden_units=[100, 50])\n",
    "  else:\n",
    "    m = tf.contrib.learn.DNNLinearCombinedClassifier(\n",
    "        model_dir=model_dir,\n",
    "        linear_feature_columns=wide_columns,\n",
    "        dnn_feature_columns=deep_columns,\n",
    "        dnn_hidden_units=[100, 50])\n",
    "  return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def input_fn(df):\n",
    "  \"\"\"Input builder function.\"\"\"\n",
    "  # Creates a dictionary mapping from each continuous feature column name (k) to\n",
    "  # the values of that column stored in a constant Tensor.\n",
    "  continuous_cols = {k: tf.constant(df[k].values) for k in CONTINUOUS_COLUMNS}\n",
    "  # Creates a dictionary mapping from each categorical feature column name (k)\n",
    "  # to the values of that column stored in a tf.SparseTensor.\n",
    "  categorical_cols = {\n",
    "      k: tf.SparseTensor(\n",
    "          indices=[[i, 0] for i in range(df[k].size)],\n",
    "          values=df[k].values,\n",
    "          shape=[df[k].size, 1])\n",
    "      for k in CATEGORICAL_COLUMNS}\n",
    "  # Merges the two dictionaries into one.\n",
    "  feature_cols = dict(continuous_cols)\n",
    "  feature_cols.update(categorical_cols)\n",
    "  # Converts the label column into a constant Tensor.\n",
    "  label = tf.constant(df[LABEL_COLUMN].values)\n",
    "  # Returns the feature columns and the label.\n",
    "  return feature_cols, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def train_and_eval():\n",
    "  \"\"\"Train and evaluate the model.\"\"\"\n",
    "  train_file_name, test_file_name = maybe_download()\n",
    "  df_train = pd.read_csv(\n",
    "      tf.gfile.Open(train_file_name),\n",
    "      names=COLUMNS,\n",
    "      skipinitialspace=True,\n",
    "      engine=\"python\")\n",
    "  df_test = pd.read_csv(\n",
    "      tf.gfile.Open(test_file_name),\n",
    "      names=COLUMNS,\n",
    "      skipinitialspace=True,\n",
    "      skiprows=1,\n",
    "      engine=\"python\")\n",
    "  # remove NaN elements\n",
    "  df_train = df_train.dropna(how='any', axis=0)\n",
    "  df_test = df_test.dropna(how='any', axis=0)\n",
    "\n",
    "  df_train[LABEL_COLUMN] = (\n",
    "      df_train[\"income_bracket\"].apply(lambda x: \">50K\" in x)).astype(int)\n",
    "  df_test[LABEL_COLUMN] = (\n",
    "      df_test[\"income_bracket\"].apply(lambda x: \">50K\" in x)).astype(int)\n",
    "\n",
    "  model_dir = gModelDir\n",
    "  print(\"model directory = %s\" % model_dir)\n",
    "\n",
    "  m = build_estimator(model_dir)\n",
    "  m.fit(input_fn=lambda: input_fn(df_train), steps=gTrainSteps)\n",
    "  results = m.evaluate(input_fn=lambda: input_fn(df_test), steps=1)\n",
    "  for key in sorted(results):\n",
    "    print(\"%s: %s\" % (key, results[key]))\n",
    "  print('Evaluated Accuracy: {0:f}'.format(results[\"accuracy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "train_and_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#  Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "#  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#  you may not use this file except in compliance with the License.\n",
    "#  You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#  Unless required by applicable law or agreed to in writing, software\n",
    "#  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#  See the License for the specific language governing permissions and\n",
    "#  limitations under the License.\n",
    "\"\"\"DNNRegressor with custom estimator for abalone dataset.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "# Learning rate for the model\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "gAbaloneDir = \"/home/data/asarcar-data/tmp/ABALONE\"\n",
    "gAbaloneModelDir = gAbaloneDir + \"/model\"\n",
    "gAbaloneDataDir = gAbaloneDir + \"/data\"\n",
    "gTrainSteps = 5000\n",
    "# Data sets\n",
    "gAbaloneTrainFileName = gAbaloneDataDir + \"/abalone_train.csv\"\n",
    "gAbaloneTestFileName = gAbaloneDataDir + \"/abalone_test.csv\"\n",
    "gAbalonePredictFileName = gAbaloneDataDir + \"/abalone_predict.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# pylint: disable=unused-argument\n",
    "def model_fn(features, targets, mode, params):\n",
    "  \"\"\"Model function for Estimator.\"\"\"\n",
    "  # Connect the first hidden layer to input layer\n",
    "  # (features) with relu activation\n",
    "  first_hidden_layer = tf.contrib.layers.relu(features, 10)\n",
    "\n",
    "  # Connect the second hidden layer to first hidden layer with relu\n",
    "  second_hidden_layer = tf.contrib.layers.relu(first_hidden_layer, 10)\n",
    "\n",
    "  # Connect the output layer to second hidden layer (no activation fn)\n",
    "  output_layer = tf.contrib.layers.linear(second_hidden_layer, 1)\n",
    "\n",
    "  # Reshape output layer to 1-dim Tensor to return predictions\n",
    "  predictions = tf.reshape(output_layer, [-1])\n",
    "  predictions_dict = {\"ages\": predictions}\n",
    "\n",
    "  # Calculate loss using mean squared error\n",
    "  loss = tf.contrib.losses.mean_squared_error(predictions, targets)\n",
    "\n",
    "  train_op = tf.contrib.layers.optimize_loss(\n",
    "      loss=loss,\n",
    "      global_step=tf.contrib.framework.get_global_step(),\n",
    "      learning_rate=params[\"learning_rate\"],\n",
    "      optimizer=\"SGD\")\n",
    "\n",
    "  return predictions_dict, loss, train_op\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def load_files():\n",
    "  # Training examples\n",
    "  train_set = tf.contrib.learn.datasets.base.load_csv_without_header(\n",
    "      filename=gAbaloneTrainFileName,\n",
    "      target_dtype=np.int,\n",
    "      features_dtype=np.float64)\n",
    "  # Test examples\n",
    "  test_set = tf.contrib.learn.datasets.base.load_csv_without_header(\n",
    "      filename=gAbaloneTestFileName,\n",
    "      target_dtype=np.int,\n",
    "      features_dtype=np.float64)\n",
    "\n",
    "  # Set of 7 examples for which to predict abalone ages\n",
    "  pred_set = tf.contrib.learn.datasets.base.load_csv_without_header(\n",
    "      filename=gAbalonePredictFileName,\n",
    "      target_dtype=np.int,\n",
    "      features_dtype=np.float64)\n",
    "\n",
    "  return train_set, test_set, pred_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Set model params\n",
    "model_params = {\"learning_rate\": LEARNING_RATE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "training_set, test_set, prediction_set = load_files()\n",
    "\n",
    "# Build 2 layer fully connected DNN with 10, 10 units respectively.\n",
    "nn = tf.contrib.learn.Estimator(\n",
    "  model_fn=model_fn, params=model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Fit\n",
    "nn.fit(x=training_set.data, y=training_set.target, steps=gTrainSteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Score accuracy\n",
    "ev = nn.evaluate(x=test_set.data, y=test_set.target, steps=1)\n",
    "loss_score = ev[\"loss\"]\n",
    "print(\"Loss: %s\" % loss_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Print out predictions\n",
    "predictions = nn.predict(x=prediction_set.data,\n",
    "                         as_iterable=True)\n",
    "for i, p in enumerate(predictions):\n",
    "  print(\"Prediction %s: %s\" % (i + 1, p[\"ages\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "name": "python2"
  },
  "name": "Contrib.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
