{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# BASIC SoftMax Classifier\n",
    "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"A very simple MNIST classifier.\n",
    "\n",
    "See extensive documentation at\n",
    "http://tensorflow.org/tutorials/mnist/beginners/index.md\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import math\n",
    "# pylint: disable=missing-docstring\n",
    "import os.path\n",
    "import sys\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# MNIST Globals\n",
    "# The MNIST dataset has 10 classes, representing the digits 0 through 9.\n",
    "gNUM_CLASSES = 10\n",
    "\n",
    "# The MNIST images are always 28x28 pixels.\n",
    "gIMAGE_SIZE = 28\n",
    "gIMAGE_PIXELS = gIMAGE_SIZE * gIMAGE_SIZE\n",
    "\n",
    "# Parameter Globals\n",
    "gMnistDir = \"/home/data/asarcar-data/tmp/MNIST\"\n",
    "# 'Initial learning rate.'\n",
    "gInputDataDir           =  gMnistDir + \"/data\"\n",
    "# 'Number of steps to run trainer.'\n",
    "gLogDir             = gMnistDir + \"/logs\"\n",
    "# 'Number of units in hidden layer 1.'\n",
    "gLearningRate   = 0.01\n",
    "# 'Number of units in hidden layer 2.'\n",
    "gMaxSteps       = 2000\n",
    "# 'Batch size.  Must divide evenly into the dataset sizes.'\n",
    "gHidden1        = 128\n",
    "# 'Directory to put the input data.'\n",
    "gHidden2        = 32\n",
    "# 'Directory to put the log data.'\n",
    "gBatchSize      = 100\n",
    "# 'If true, uses fake data for unit testing.',\n",
    "gFakeData       = False\n",
    "# 'Keep probability for training dropout.'\n",
    "gDropOut = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Import Data\n",
    "mnist_data_dir = gInputDataDir\n",
    "mnist_data = input_data.read_data_sets(mnist_data_dir, one_hot=True, fake_data=gFakeData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#  Convolutional Neural Net Classifier\n",
    "# Noise in W breaks symmetry. Otherwise: backpropagation does not\n",
    "# function: Error(l) = W'(l+1)*errors(l+1)*grad(f(l)) = 0. \n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape,stddev=0.1)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# ReLU require +ve initial bias to avoid \"dead\" neurons.\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1,shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Input placeholders\n",
    "with tf.name_scope('input'):\n",
    "  # Create the model\n",
    "  x = tf.placeholder(tf.float32, [None, 784])\n",
    "  # Define loss and optimizer\n",
    "  y_ = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "y = tf.matmul(x, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# The raw formulation of cross-entropy,\n",
    "#\n",
    "#   tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(tf.nn.softmax(y)),\n",
    "#                                 reduction_indices=[1]))\n",
    "#\n",
    "# can be numerically unstable.\n",
    "#\n",
    "# So here we use tf.nn.softmax_cross_entropy_with_logits on the raw\n",
    "# outputs of 'y', and then average across the batch.\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, y_))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "# Train\n",
    "for _ in range(1000):\n",
    "  batch_xs, batch_ys = mnist_data.train.next_batch(100)\n",
    "  sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9171\n"
     ]
    }
   ],
   "source": [
    "# Test trained model\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(sess.run(accuracy, feed_dict={x: mnist_data.test.images,\n",
    "\t\t\t\t    y_: mnist_data.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# convolutions uses a stride of one and are zero padded\n",
    "# output is the same size as the input. \n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# plain old max pooling over 2x2 blocks in 2nd & 3rd Dimension\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Reshape image to a 4D Tensor (D1,D2,D3,D4) == (N,28,28,1)\n",
    "# where N is number of images submitted in the batch\n",
    "# -1 used to infer the shape to map to same tensor size\n",
    "x_image = tf.reshape(x, [-1,28,28,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# First Convolution Layer: (5,5,1) => 32 features\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "# Convole image(W,b) => ReLU => MaxPool\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "# Image Pooled: (N,28,28,32) => (N,14,14,32)\n",
    "h_pool1 = max_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Second Convolution Layer: (5,5,32) => 64 features\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "# Convole image(W,b) => ReLU => MaxPool\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "# Image Pooled: (N,14,14,64) => (N,7,7,64)\n",
    "h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Fully Connected Layer: (7,7,64) => 1024 features\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "# Reshape to N vector(s) => (W,b) => ReLU\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Apply Dropout\n",
    "# placeholder: allows turning on during training and off during testing.\n",
    "# tf.nn.dropout: automatically handles scaling neuron outputs\n",
    "# in addition to masking. so dropout works without any additional scaling.\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Final Output Layer: (N,1024) => (W,b) => (N,10)\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# The raw formulation of cross-entropy,\n",
    "#\n",
    "#   tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(tf.nn.softmax(y)),\n",
    "#                                 reduction_indices=[1]))\n",
    "#\n",
    "# can be numerically unstable.\n",
    "#\n",
    "# So here we use tf.nn.softmax_cross_entropy_with_logits on the raw\n",
    "# outputs of 'y', and then average across the batch.\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_conv, y_))\n",
    "# Use ADAM Optimizer (step size smartly taken)\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 18000, training accuracy 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 16000, training accuracy 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 14000, training accuracy 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 12000, training accuracy 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10000, training accuracy 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 8000, training accuracy 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 6000, training accuracy 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4000, training accuracy 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2000, training accuracy 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.18\n"
     ]
    }
   ],
   "source": [
    "log_iter = 2000\n",
    "max_iter = 20000\n",
    "# Train\n",
    "for i in range(max_iter):\n",
    "  batch = mnist_data.train.next_batch(50)\n",
    "  # sess.run(train_step, feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5}) \n",
    "  train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "  # Feed: Dropout probability when feeding Tensor Computation Graph\n",
    "  # Log every 100th iteration during training\n",
    "  if i%log_iter == 0:\n",
    "    train_accuracy = accuracy.eval(feed_dict={\n",
    "      x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "    print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 0.9926\n"
     ]
    }
   ],
   "source": [
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "  x: mnist_data.test.images, y_: mnist_data.test.labels, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Builds the MNIST network.\\nImplements the inference/loss/training pattern for model building.\\n1. inference() - Builds the model as far as is required for running the network\\nforward to make predictions.\\n2. loss() - Adds to the inference model the layers required to generate loss.\\n3. training() - Adds to the loss model the Ops required to generate and\\napply gradients.\\n\\nThis file is used by the various \"fully_connected_*.py\" files and not meant to\\nbe run.\\n'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  2 Stage Neural Net + Softmax Classifier\n",
    "\n",
    "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"Builds the MNIST network.\n",
    "Implements the inference/loss/training pattern for model building.\n",
    "1. inference() - Builds the model as far as is required for running the network\n",
    "forward to make predictions.\n",
    "2. loss() - Adds to the inference model the layers required to generate loss.\n",
    "3. training() - Adds to the loss model the Ops required to generate and\n",
    "apply gradients.\n",
    "\n",
    "This file is used by the various \"fully_connected_*.py\" files and not meant to\n",
    "be run.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/data/asarcar-data/tmp/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting /home/data/asarcar-data/tmp/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /home/data/asarcar-data/tmp/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/data/asarcar-data/tmp/MNIST_data/train-images-idx3-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist_data = input_data.read_data_sets(mnist_data_dir, gFakeData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def inference(images, hidden1_units, hidden2_units):\n",
    "  \"\"\"Build the MNIST model up to where it may be used for inference.\n",
    "  Args:\n",
    "    images: Images placeholder, from inputs().\n",
    "    hidden1_units: Size of the first hidden layer.\n",
    "    hidden2_units: Size of the second hidden layer.\n",
    "\n",
    "  Returns:\n",
    "    softmax_linear: Output tensor with the computed logits.\n",
    "  \"\"\"\n",
    "  # Hidden 1\n",
    "  with tf.name_scope('hidden1'):\n",
    "    wt_init = tf.truncated_normal([gIMAGE_PIXELS, hidden1_units],\n",
    "\t\t\t\t  stddev=1.0 / math.sqrt(float(gIMAGE_PIXELS)))\n",
    "    weights = tf.Variable(wt_init, name='weights')\n",
    "    bi_init = tf.zeros([hidden1_units])\n",
    "    biases = tf.Variable(bi_init, name='biases')\n",
    "    hidden1 = tf.nn.relu(tf.matmul(images, weights) + biases)\n",
    "  # Hidden 2\n",
    "  with tf.name_scope('hidden2'):\n",
    "    wt_init = tf.truncated_normal([hidden1_units, hidden2_units],\n",
    "\t\t\t\t  stddev=1.0 / math.sqrt(float(hidden1_units)))\n",
    "    weights = tf.Variable(wt_init, name='weights')\n",
    "    bi_init = tf.zeros([hidden2_units])\n",
    "    biases = tf.Variable(bi_init, name='biases')\n",
    "    hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n",
    "  # Linear\n",
    "  with tf.name_scope('softmax_linear'):\n",
    "    wt_init = tf.truncated_normal([hidden2_units, gNUM_CLASSES],\n",
    "\t\t\t\t  stddev=1.0 / math.sqrt(float(hidden2_units)))\n",
    "    weights = tf.Variable(wt_init, name='weights')\n",
    "    bi_init = tf.zeros([gNUM_CLASSES]),\n",
    "    biases = tf.Variable(bi_init, name='biases')\n",
    "    logits_op = tf.matmul(hidden2, weights) + biases\n",
    "  return logits_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def loss(logits, labels):\n",
    "  \"\"\"Calculates the loss from the logits and the labels.\n",
    "  Args:\n",
    "    logits: Logits tensor, float - [batch_size, gNUM_CLASSES].\n",
    "    labels: Labels tensor, int32 - [batch_size].\n",
    "\n",
    "  Returns:\n",
    "    loss: Loss tensor of type float.\n",
    "  \"\"\"\n",
    "  labels = tf.to_int64(labels)\n",
    "  cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "      logits, labels, name='xentropy')\n",
    "  loss_op = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "  return loss_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def training(loss_op, learning_rate):\n",
    "  \"\"\"Sets up the training Ops.\n",
    "  Creates a summarizer to track the loss over time in TensorBoard.\n",
    "\n",
    "  Creates an optimizer and applies the gradients to all trainable variables.\n",
    "\n",
    "  The Op returned by this function is what must be passed to the\n",
    "  `sess.run()` call to cause the model to train.\n",
    "\n",
    "  Args:\n",
    "    loss: Loss tensor, from loss().\n",
    "    learning_rate: The learning rate to use for gradient descent.\n",
    "\n",
    "  Returns:\n",
    "    train_op: The Op for training.\n",
    "  \"\"\"\n",
    "  # Add a scalar summary for the snapshot loss.\n",
    "  # tf.summary.scalar('loss', loss_op)\n",
    "  tf.summary.scalar(loss_op.op.name, loss_op)\n",
    "  # Create the gradient descent optimizer with the given learning rate.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  # Create a variable to track the global step.\n",
    "  global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "  # Use the optimizer to apply the gradients that minimize the loss\n",
    "  # (and also increment the global step counter) as a single training step.\n",
    "  train_op = optimizer.minimize(loss_op, global_step=global_step)\n",
    "  return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def evaluation(logits, labels):\n",
    "  \"\"\"Evaluate the quality of the logits at predicting the label.\n",
    "\n",
    "  Args:\n",
    "    logits: Logits tensor, float - [batch_size, gNUM_CLASSES].\n",
    "    labels: Labels tensor, int32 - [batch_size], with values in the\n",
    "      range [0, gNUM_CLASSES).\n",
    "\n",
    "  Returns:\n",
    "    A scalar int32 tensor with the number of examples (out of batch_size)\n",
    "    that were predicted correctly.\n",
    "  \"\"\"\n",
    "  # For a classifier model, we can use the in_top_k Op.\n",
    "  # It returns a bool tensor with shape [batch_size] that is true for\n",
    "  # the examples where the label is in the top k (here k=1)\n",
    "  # of all logits for that example.\n",
    "  correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "  # Return the number of true entries.\n",
    "  return tf.reduce_sum(tf.cast(correct, tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def placeholder_inputs(batch_size):\n",
    "  \"\"\"Generate placeholder variables to represent the input tensors.\n",
    "  These placeholders are used as inputs by the rest of the model building\n",
    "  code and will be fed from the downloaded data in the .run() loop, below.\n",
    "  Args:\n",
    "    batch_size: The batch size will be baked into both placeholders.\n",
    "\n",
    "  Returns:\n",
    "    images_placeholder: Images placeholder.\n",
    "    labels_placeholder: Labels placeholder.\n",
    "  \"\"\"\n",
    "  # Note that the shapes of the placeholders match the shapes of the full\n",
    "  # image and label tensors, except the first dimension is now batch_size\n",
    "  # rather than the full size of the train or test data sets.\n",
    "  images_placeholder = tf.placeholder(tf.float32,\n",
    "\t\t\t\t      shape=(batch_size, gIMAGE_PIXELS))\n",
    "  labels_placeholder = tf.placeholder(tf.int32, shape=(batch_size))\n",
    "  return images_placeholder, labels_placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def fill_feed_dict(data_set, images_pl, labels_pl):\n",
    "  \"\"\"Fills the feed_dict for training the given step.\n",
    "  A feed_dict takes the form of:\n",
    "  feed_dict = {\n",
    "      <placeholder>: <tensor of values to be passed for placeholder>,\n",
    "      ....\n",
    "  }\n",
    "\n",
    "  Args:\n",
    "    data_set: The set of images and labels, from input_data.read_data_sets()\n",
    "    images_pl: The images placeholder, from placeholder_inputs().\n",
    "    labels_pl: The labels placeholder, from placeholder_inputs().\n",
    "\n",
    "  Returns:\n",
    "    feed_dict: The feed dictionary mapping from placeholders to values.\n",
    "  \"\"\"\n",
    "  # Create the feed_dict for the placeholders filled with the next\n",
    "  # `batch size` examples.\n",
    "  images_feed, labels_feed = data_set.next_batch(gBatchSize, gFakeData)\n",
    "  feed_dict = {\n",
    "      images_pl: images_feed,\n",
    "      labels_pl: labels_feed,\n",
    "  }\n",
    "  return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def do_eval(sess,\n",
    "            eval_correct,\n",
    "            images_placeholder,\n",
    "            labels_placeholder,\n",
    "            data_set):\n",
    "  \"\"\"Runs one evaluation against the full epoch of data.\n",
    "\n",
    "  Args:\n",
    "    sess: The session in which the model has been trained.\n",
    "    eval_correct: The Tensor that returns the number of correct predictions.\n",
    "    images_placeholder: The images placeholder.\n",
    "    labels_placeholder: The labels placeholder.\n",
    "    data_set: The set of images and labels to evaluate, from\n",
    "      input_data.read_data_sets().\n",
    "  \"\"\"\n",
    "  # And run one epoch of eval.\n",
    "  true_count = 0  # Counts the number of correct predictions.\n",
    "  steps_per_epoch = data_set.num_examples // gBatchSize\n",
    "  num_examples = steps_per_epoch * gBatchSize\n",
    "  for step in xrange(steps_per_epoch):\n",
    "    feed_dict = fill_feed_dict(data_set,\n",
    "                               images_placeholder,\n",
    "                               labels_placeholder)\n",
    "    true_count += sess.run(eval_correct, feed_dict=feed_dict)\n",
    "  precision = float(true_count) / num_examples\n",
    "  print('  Num examples: %d  Num correct: %d  Precision @ 1: %0.04f' %\n",
    "        (num_examples, true_count, precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def run_training():\n",
    "  \"\"\"Train MNIST for a number of steps.\"\"\"\n",
    "  # Get the sets of images and labels for training, validation, and\n",
    "  # test on MNIST.\n",
    "  # Tell TensorFlow that the model will be built into the default Graph.\n",
    "  with tf.Graph().as_default():\n",
    "    # Generate placeholders for the images and labels.\n",
    "    images_placeholder, labels_placeholder = placeholder_inputs(gBatchSize)\n",
    "\n",
    "    # Build a Graph that computes predictions from the inference model.\n",
    "    logits_op = inference(images_placeholder, gHidden1, gHidden2)\n",
    "\n",
    "    # Add to the Graph the Ops for loss calculation.\n",
    "    loss_op = loss(logits_op, labels_placeholder)\n",
    "\n",
    "    # Add to the Graph the Ops that calculate and apply gradients.\n",
    "    train_op = training(loss_op, gLearningRate)\n",
    "\n",
    "    # Add the Op to compare the logits to the labels during evaluation.\n",
    "    eval_correct_op = evaluation(logits_op, labels_placeholder)\n",
    "\n",
    "    # Build the summary Tensor based on the TF collection of Summaries.\n",
    "    summary = tf.summary.merge_all()\n",
    "\n",
    "    # Add the variable initializer Op.\n",
    "    init_op = tf.global_variables_initializer()\n",
    "\n",
    "    # Create a saver for writing training checkpoints.\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    # Create a session for running Ops on the Graph.\n",
    "    sess = tf.Session()\n",
    "\n",
    "    # Instantiate a SummaryWriter to output summaries and the Graph.\n",
    "    summary_writer = tf.summary.FileWriter(gLogDir, sess.graph)\n",
    "\n",
    "    # And then after everything is built:\n",
    "\n",
    "    # Run the Op to initialize the variables.\n",
    "    sess.run(init_op)\n",
    "\n",
    "    # Start the training loop.\n",
    "    for step in xrange(gMaxSteps):\n",
    "      start_time = time.time()\n",
    "\n",
    "      # Fill a feed dictionary with the actual set of images and labels\n",
    "      # for this particular training step.\n",
    "      feed_dict = fill_feed_dict(mnist_data.train,\n",
    "                                 images_placeholder,\n",
    "                                 labels_placeholder)\n",
    "\n",
    "      # Run one step of the model.  The return values are the activations\n",
    "      # from the `train_op` (which is discarded) and the `loss` Op.  To\n",
    "      # inspect the values of your Ops or variables, you may include them\n",
    "      # in the list passed to sess.run() and the value tensors will be\n",
    "      # returned in the tuple from the call.\n",
    "      _, loss_value = sess.run([train_op, loss_op],\n",
    "                               feed_dict=feed_dict)\n",
    "\n",
    "      duration = time.time() - start_time\n",
    "\n",
    "      # Write the summaries and print an overview fairly often.\n",
    "      if step % 100 == 0:\n",
    "        # Print status to stdout.\n",
    "        print('Step %d: loss = %.2f (%.3f sec)' % (step, loss_value, duration))\n",
    "        # Update the events file.\n",
    "        summary_str = sess.run(summary, feed_dict=feed_dict)\n",
    "        summary_writer.add_summary(summary_str, step)\n",
    "        summary_writer.flush()\n",
    "\n",
    "      # Save a checkpoint and evaluate the model periodically.\n",
    "      if (step + 1) % 1000 == 0 or (step + 1) == gMaxSteps:\n",
    "        checkpoint_file = os.path.join(gLogDir, 'model.ckpt')\n",
    "        saver.save(sess, checkpoint_file, global_step=step)\n",
    "        # Evaluate against the training set.\n",
    "        print('Training Data Eval:')\n",
    "        do_eval(sess,\n",
    "                eval_correct_op,\n",
    "                images_placeholder,\n",
    "                labels_placeholder,\n",
    "                mnist_data.test)\n",
    "        # Evaluate against the validation set.\n",
    "        print('Validation Data Eval:')\n",
    "        do_eval(sess,\n",
    "                eval_correct_op,\n",
    "                images_placeholder,\n",
    "                labels_placeholder,\n",
    "                mnist_data.validation)\n",
    "        # Evaluate against the test set.\n",
    "        print('Test Data Eval:')\n",
    "        do_eval(sess,\n",
    "                eval_correct_op,\n",
    "                images_placeholder,\n",
    "                labels_placeholder,\n",
    "                mnist_data.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def set_up():\n",
    "  if tf.gfile.Exists(gLogDir):\n",
    "    tf.gfile.DeleteRecursively(gLogDir)\n",
    "  tf.gfile.MakeDirs(gLogDir)\n",
    "  run_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Num examples: 10000  Num correct: 8966  Precision @ 1: 0.8966\n",
      "Validation Data Eval:\n",
      "  Num examples: 5000  Num correct: 4506  Precision @ 1: 0.9012\n",
      "Test Data Eval:\n",
      "  Num examples: 10000  Num correct: 8966  Precision @ 1: 0.8966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1800: loss = 0.30 (0.001 sec)\n",
      "Step 1900: loss = 0.29 (0.001 sec)\n",
      "Training Data Eval:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1500: loss = 0.44 (0.001 sec)\n",
      "Step 1600: loss = 0.47 (0.001 sec)\n",
      "Step 1700: loss = 0.39 (0.001 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1100: loss = 0.60 (0.024 sec)\n",
      "Step 1200: loss = 0.37 (0.001 sec)\n",
      "Step 1300: loss = 0.32 (0.001 sec)\n",
      "Step 1400: loss = 0.41 (0.001 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Eval:\n",
      "  Num examples: 10000  Num correct: 8699  Precision @ 1: 0.8699\n",
      "Validation Data Eval:\n",
      "  Num examples: 5000  Num correct: 4340  Precision @ 1: 0.8680\n",
      "Test Data Eval:\n",
      "  Num examples: 10000  Num correct: 8699  Precision @ 1: 0.8699\n",
      "Step 1000: loss = 0.43 (0.001 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 700: loss = 0.62 (0.001 sec)\n",
      "Step 800: loss = 0.56 (0.001 sec)\n",
      "Step 900: loss = 0.58 (0.001 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 400: loss = 1.27 (0.001 sec)\n",
      "Step 500: loss = 0.82 (0.001 sec)\n",
      "Step 600: loss = 0.72 (0.001 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: loss = 2.30 (0.004 sec)\n",
      "Step 100: loss = 2.09 (0.001 sec)\n",
      "Step 200: loss = 1.83 (0.001 sec)\n",
      "Step 300: loss = 1.50 (0.001 sec)\n"
     ]
    }
   ],
   "source": [
    "# tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n",
    "set_up()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A simple MNIST classifier which displays summaries in TensorBoard.\\n This is an unimpressive MNIST model, but it is a good example of using\\ntf.name_scope to make a graph legible in the TensorBoard graph explorer, and of\\nnaming summary tags so that they are grouped meaningfully in TensorBoard.\\nIt demonstrates the functionality of every TensorBoard dashboard.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the 'License');\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an 'AS IS' BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"A simple MNIST classifier which displays summaries in TensorBoard.\n",
    " This is an unimpressive MNIST model, but it is a good example of using\n",
    "tf.name_scope to make a graph legible in the TensorBoard graph explorer, and of\n",
    "naming summary tags so that they are grouped meaningfully in TensorBoard.\n",
    "It demonstrates the functionality of every TensorBoard dashboard.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/data/asarcar-data/tmp/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting /home/data/asarcar-data/tmp/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting /home/data/asarcar-data/tmp/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /home/data/asarcar-data/tmp/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist_data_dir = \"/home/data/asarcar-data/tmp/MNIST_data\"\n",
    "mnist_data = input_data.read_data_sets(mnist_data_dir, one_hot=True, fake_data=gFakeData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def variable_summaries(var):\n",
    "  \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "  with tf.name_scope('summaries'):\n",
    "    mean = tf.reduce_mean(var)\n",
    "    tf.summary.scalar('mean', mean)\n",
    "    with tf.name_scope('stddev'):\n",
    "      stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "    tf.summary.scalar('stddev', stddev)\n",
    "    tf.summary.scalar('max', tf.reduce_max(var))\n",
    "    tf.summary.scalar('min', tf.reduce_min(var))\n",
    "    tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def nn_layer(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu):\n",
    "  \"\"\"Reusable code for making a simple neural net layer.\n",
    "  It does a matrix multiply, bias add, and then uses relu to nonlinearize.\n",
    "  It also sets up name scoping so that the resultant graph is easy to read,\n",
    "  and adds a number of summary ops.\n",
    "  \"\"\"\n",
    "  # Adding a name scope ensures logical grouping of the layers in the graph.\n",
    "  with tf.name_scope(layer_name):\n",
    "    # This Variable will hold the state of the weights for the layer\n",
    "    with tf.name_scope('weights'):\n",
    "      weights = weight_variable([input_dim, output_dim])\n",
    "      variable_summaries(weights)\n",
    "    with tf.name_scope('biases'):\n",
    "      biases = bias_variable([output_dim])\n",
    "      variable_summaries(biases)\n",
    "    with tf.name_scope('Wx_plus_b'):\n",
    "      preactivate = tf.matmul(input_tensor, weights) + biases\n",
    "      tf.summary.histogram('pre_activations', preactivate)\n",
    "    activations = act(preactivate, name='activation')\n",
    "    tf.summary.histogram('activations', activations)\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "  sess = tf.InteractiveSession()\n",
    "\n",
    "  # Define Layers\n",
    "  # Input\n",
    "  # Create a multilayer model.\n",
    "  with tf.name_scope('input_reshape'):\n",
    "    image_shaped_input = tf.reshape(x, [-1, 28, 28, 1])\n",
    "    tf.summary.image('input', image_shaped_input, 10)\n",
    "  \n",
    "  hidden1 = nn_layer(x, 784, 500, 'layer1')\n",
    "\n",
    "  with tf.name_scope('dropout'):\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    tf.summary.scalar('dropout_keep_probability', keep_prob)\n",
    "    dropped = tf.nn.dropout(hidden1, keep_prob)\n",
    "\n",
    "  # Do not apply softmax activation yet, see below.\n",
    "  y = nn_layer(dropped, 500, 10, 'layer2', act=tf.identity)\n",
    "\n",
    "  with tf.name_scope('cross_entropy'):\n",
    "    # The raw formulation of cross-entropy,\n",
    "    #\n",
    "    # tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(tf.softmax(y)),\n",
    "    #                               reduction_indices=[1]))\n",
    "    #\n",
    "    # can be numerically unstable.\n",
    "    #\n",
    "    # So here we use tf.nn.softmax_cross_entropy_with_logits on the\n",
    "    # raw outputs of the nn_layer above, and then average across\n",
    "    # the batch.\n",
    "    diff = tf.nn.softmax_cross_entropy_with_logits(y, y_)\n",
    "    with tf.name_scope('total'):\n",
    "      cross_entropy = tf.reduce_mean(diff)\n",
    "  tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "\n",
    "  with tf.name_scope('train'):\n",
    "    train_step = tf.train.AdamOptimizer(gLearningRate).minimize(\n",
    "        cross_entropy)\n",
    "\n",
    "  with tf.name_scope('accuracy'):\n",
    "    with tf.name_scope('correct_prediction'):\n",
    "      correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    with tf.name_scope('accuracy'):\n",
    "      accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "  tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "  # Merge all the summaries and write them out to /tmp/mnist_logs (by default)\n",
    "  merged = tf.summary.merge_all()\n",
    "  train_writer = tf.summary.FileWriter(gLogDir + '/train', sess.graph)\n",
    "  test_writer = tf.summary.FileWriter(gLogDir + '/test')\n",
    "\n",
    "  init_op = tf.global_variables_initializer()\n",
    "  sess.run(init_op)\n",
    "\n",
    "  def feed_dict(train):\n",
    "    \"\"\"Make a TensorFlow feed_dict: maps data onto Tensor placeholders.\"\"\"\n",
    "    if train or gFakeData:\n",
    "      xs, ys = mnist_data.train.next_batch(100, fake_data=gFakeData)\n",
    "      k = gDropOut\n",
    "    else:\n",
    "      xs, ys = mnist_data.test.images, mnist_data.test.labels\n",
    "      k = 1.0\n",
    "    return {x: xs, y_: ys, keep_prob: k}\n",
    "\n",
    "  # Train the model, and also write summaries.\n",
    "  # Every 10th step, measure test-set accuracy, and write test summaries\n",
    "  # All other steps, run train_step on training data, & add training summaries\n",
    "  for i in range(gMaxSteps):\n",
    "    if i % 10 == 0:  # Record summaries and test-set accuracy\n",
    "      summary, acc = sess.run([merged, accuracy], feed_dict=feed_dict(False))\n",
    "      test_writer.add_summary(summary, i)\n",
    "      print('Accuracy at step %s: %s' % (i, acc))\n",
    "    else:  # Record train set summaries, and train\n",
    "      if i % 100 == 99:  # Record execution stats\n",
    "        run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "        run_metadata = tf.RunMetadata()\n",
    "        summary, _ = sess.run([merged, train_step],\n",
    "                              feed_dict=feed_dict(True),\n",
    "                              options=run_options,\n",
    "                              run_metadata=run_metadata)\n",
    "        train_writer.add_run_metadata(run_metadata, 'step%03d' % i)\n",
    "        train_writer.add_summary(summary, i)\n",
    "        print('Adding run metadata for', i)\n",
    "      else:  # Record a summary\n",
    "        summary, _ = sess.run([merged, train_step], feed_dict=feed_dict(True))\n",
    "        train_writer.add_summary(summary, i)\n",
    "  train_writer.close()\n",
    "  test_writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def set_up():\n",
    "  if tf.gfile.Exists(gLogDir):\n",
    "    tf.gfile.DeleteRecursively(gLogDir)\n",
    "  tf.gfile.MakeDirs(gLogDir)\n",
    "  train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "set_up()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "name": "python2"
  },
  "name": "HandWriting.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
